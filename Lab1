
def tokenize(lines):

    words=[]

    for line in lines:

        start = 0
        while start < len(line):
            
            while line[start].isspace():
                start=start+1
                 
            #print(line[start])
            
            if line[start].isalpha():
                #print(line[start] + " is a letter")
                
                end = start
                
                while line[end].isalpha():
                    
                    end = end+1
                    
                words.append(line[start:end])    
                start = end
                
            elif line[start].isdigit():
                #print(line[start] + " is a digit")
                
                end = start
                
                while line[end].isdigit():
                    
                    end = end+1
                    
                words.append(line[start:end])    
                start = end
                
                #print(line[start] + " is a symbol")
            else:
                
                words.append(line[start])
                         
                start = start+1
        
    return words
    


#print(tokenize(['10 sweet apple tarts!']))


def countWords(words, stopWords):
  
    frequencies={}
    
    count=0
    
    for word in words:
        
        
        if word not in stopWords:
            
            if word not in frequencies:
                    
                count = 1
                frequencies.update({word})
                
            else:
                count=count+1
    
    return frequencies



print(countWords(['it','is','a','book'], ['a','is','it'] ))        
                



