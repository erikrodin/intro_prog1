


def tokenize(lines):

    words=[]

    for line in lines:

        start = 0
        while start < len(line):
            
            while line[start].isspace():
                start=start+1
                 
            #print(line[start])
            
            if line[start].isalpha():
                #print(line[start] + " is a letter")
                
                end = start
                
                while line[end].isalpha():
                    
                    end = end+1
                    
                words.append(line[start:end])    
                start = end
                
            elif line[start].isdigit():
                #print(line[start] + " is a digit")
                
                end = start
                
                while line[end].isdigit():
                    
                    end = end+1
                    
                words.append(line[start:end])    
                start = end
                
                #print(line[start] + " is a symbol")
            else:
                
                words.append(line[start])
                         
            start = start+1
        
    return words
    


print(tokenize(['10  sweet  apple  tarts.']))

